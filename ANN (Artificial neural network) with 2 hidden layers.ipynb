{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3716bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "  def __init__(self, input_dim, output_dim, alpha, activation_func, num_of_epoch, weight_initial, hidden_dim1, hidden_dim2):\n",
    "    self.input_dim = input_dim # The number of input features.\n",
    "    self.output_dim = output_dim # The number of output classes.\n",
    "    self.alpha = alpha # The learning rate.\n",
    "    self.cost_list = [] # To store cost after each epoch.\n",
    "    self.num_of_epoch = num_of_epoch # The number of epochs to train the network.\n",
    "    \n",
    "    # hidden_dim1, hidden_dim2 represent the number of hidden units in the first and second hidden layers, respectively.\n",
    "\n",
    "    # The activation function to use, can be one of 'relu', 'sigmoid', or 'softmax'.\n",
    "    if activation_func == 'relu':\n",
    "      self.activation_func = self.relu\n",
    "      self.deriv_activation_func = self.deriv_relu\n",
    "    elif activation_func == 'softmax':\n",
    "      self.activation_func = self.softmax\n",
    "      self.deriv_activation_func = self.deriv_softmax\n",
    "    elif activation_func == 'sigmoid':\n",
    "      self.activation_func = self.sigmoid\n",
    "      self.deriv_activation_func = self.deriv_sigmoid\n",
    "    \n",
    "    np.random.seed(0) # to initialise all the weights with the same random values every time the code is executed.\n",
    "\n",
    "    # The weight initialization method can be one of 'random', 'zero', or 'constant'.\n",
    "    if weight_initial == 'random':\n",
    "      self.w1 = np.random.rand(input_dim, hidden_dim1)\n",
    "      self.b1 = np.zeros((1, hidden_dim1))\n",
    "\n",
    "      self.w2 = np.random.rand(hidden_dim1, hidden_dim2)\n",
    "      self.b2 = np.zeros((1, hidden_dim2))\n",
    "\n",
    "      self.w3 = np.random.rand(hidden_dim2, output_dim)\n",
    "      self.b3 = np.zeros((1, output_dim))\n",
    "\n",
    "    elif weight_initial == 'zero':\n",
    "      self.w1 = np.zeros((input_dim, hidden_dim1))\n",
    "      self.b1 = np.zeros((1, hidden_dim1))\n",
    "\n",
    "      self.w2 = np.zeros((hidden_dim1, hidden_dim2))\n",
    "      self.b2 = np.zeros((1, hidden_dim2))\n",
    "\n",
    "      self.w3 = np.zeros((hidden_dim2, output_dim))\n",
    "      self.b3 = np.zeros((1, output_dim))\n",
    "\n",
    "    elif weight_initial == 'constant':\n",
    "      self.w1 = np.ones((input_dim, hidden_dim1)) * 0.5\n",
    "      self.b1 = np.zeros((1, hidden_dim1))\n",
    "\n",
    "      self.w2 = np.ones((hidden_dim1, hidden_dim2)) * 0.5\n",
    "      self.b2 = np.zeros((1, hidden_dim2))\n",
    "\n",
    "      self.w3 = np.ones((hidden_dim2, output_dim)) * 0.5\n",
    "      self.b3 = np.zeros((1, output_dim))\n",
    "\n",
    "  def relu(self, Z): # The ReLU activation function.\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "  def sigmoid(self, Z): # The sigmoid activation function.\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "  def softmax(self, Z): # The softmax activation function.\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "  def deriv_softmax(self, Z): # The derivative of the softmax activation function.\n",
    "    s = self.softmax(Z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "  def deriv_relu(self, Z): # The derivative of the ReLU activation function.\n",
    "    return Z > 0\n",
    "\n",
    "  def deriv_sigmoid(self, Z): # The derivative of the sigmoid activation function.\n",
    "    s = self.sigmoid(Z)\n",
    "    return s*(1-s)\n",
    "\n",
    "  def forward(self, X): # Forward propagates the input through the network.\n",
    "    self.z1 = np.matmul(X, self.w1) + self.b1\n",
    "    self.a1 = self.activation_func(self.z1)\n",
    "\n",
    "    self.z2 = np.matmul(self.z1, self.w2) + self.b2\n",
    "    self.a2 = self.activation_func(self.z2)\n",
    "\n",
    "    self.z3 = np.matmul(self.z2, self.w3) + self.b3\n",
    "    self.a3 = self.softmax(self.z3)\n",
    "\n",
    "  def one_hot(self, y): # Converts a target vector to matrix of 1 and 0 using one-hot encoding.\n",
    "    one_hot_y = np.zeros((y.size, (y.max()+1)))\n",
    "    one_hot_y[np.arange(y.size), y] = 1\n",
    "    return one_hot_y\n",
    "\n",
    "  def backward(self, X, y): # Backward propagates the error and computes the gradients.\n",
    "    m = y.shape[1]\n",
    "\n",
    "    cost = np.sum((self.a3 - y)**2, axis=1).mean()\n",
    "    self.cost_list.append(cost)\n",
    "\n",
    "    dw3 = (1/m)*(np.sum(self.a3 - y, axis=1).mean()) * (np.matmul(self.a2.T, self.deriv_softmax(self.z3)))\n",
    "    db3 = (1/m)*(np.sum(self.a3 - y, axis=1).mean()) * (self.deriv_softmax(self.z3))\n",
    "    db3 = np.mean(db3, axis=0).reshape(1, -1)\n",
    "\n",
    "    dw2 = (1/m)*(np.sum(self.a3 - y, axis=1).mean()) * np.matmul(self.a1.T, np.matmul(self.deriv_softmax(self.z3), self.w3.T) * self.deriv_activation_func(self.z2))\n",
    "    db2 = (1/m)*(np.sum(self.a3 - y, axis=1).mean()) * np.matmul(self.deriv_softmax(self.z3), self.w3.T) * self.deriv_activation_func(self.z2)\n",
    "    db2 = np.mean(db2, axis=0).reshape(1, -1)\n",
    "\n",
    "    dw1 = (1/m)*(np.sum(self.a3 - y, axis=1).mean()) * np.matmul(X.T, np.matmul(np.matmul(self.deriv_softmax(self.z3), self.w3.T) * self.deriv_activation_func(self.z2), self.w2.T) * self.deriv_activation_func(self.z1))\n",
    "    db1 = (1/m)*(np.sum(self.a3 - y, axis=1).mean()) * np.matmul(np.matmul(self.deriv_softmax(self.z3), self.w3.T) * self.deriv_activation_func(self.z2), self.w2.T) * self.deriv_activation_func(self.z1)\n",
    "    db1 = np.mean(db1, axis=0).reshape(1, -1)\n",
    "    \n",
    "    return dw1, db1, dw2, db2, dw3, db3\n",
    "\n",
    "  def update_params(self, dw1, db1, dw2, db2, dw3, db3): # Updates the parameters obtained from backward method.\n",
    "    self.w1 = self.w1 - self.alpha*dw1\n",
    "    self.b1 = self.b1 - self.alpha*db1\n",
    "\n",
    "    self.w2 = self.w2 - self.alpha*dw2\n",
    "    self.b2 = self.b2 - self.alpha*db2\n",
    "\n",
    "    self.w3 = self.w3 - self.alpha*dw3\n",
    "    self.b3 = self.b3 - self.alpha*db3\n",
    "\n",
    "  def stochastic_gradient_descent(self, X, y): # Trains the network using stochastic gradient descent.\n",
    "    for i in range(self.num_of_epoch):\n",
    "      self.forward(X)\n",
    "      dw1, db1, dw2, db2, dw3, db3 = self.backward(X, y)\n",
    "      self.update_params(dw1, db1, dw2, db2, dw3, db3)\n",
    "\n",
    "  def fit(self, X, y): # Fits the input in the model.\n",
    "    one_hot_y = y.copy()\n",
    "    one_hot_y = self.one_hot(one_hot_y)\n",
    "    self.stochastic_gradient_descent(X, one_hot_y)\n",
    "\n",
    "  def predict(self, X): # Predicts the output for a given test sample.\n",
    "    z1_p = np.matmul(X, self.w1) + self.b1\n",
    "    a1_p = self.relu(z1_p)\n",
    "\n",
    "    z2_p = np.matmul(a1_p, self.w2) + self.b2\n",
    "    a2_p = self.relu(z2_p)\n",
    "\n",
    "    z3_p = np.matmul(a2_p, self.w3) + self.b3\n",
    "    a3_p = self.softmax(z3_p)\n",
    "\n",
    "    return np.argmax(a3_p, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
